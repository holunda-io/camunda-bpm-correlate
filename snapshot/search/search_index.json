{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Why should I use this? # Imagine you integrate your Camunda Engine into a larger application landscape. In doing so the inter-system communication becomes important and questions on communication styles and patterns arise. In the world of self-contained systems, the asynchronous communication with messages is wide adopted. This library helps you to solve integration problems around correlation of messages with processes. How to start? # A good starting point is the Getting Started section, but then go on through the and have a look Introduction and check out our Working Examples . If you need more details on usage and configuration, check the User Guide . Get in touch # If you are missing a feature, have a question regarding usage or deployment, you should definitely get in touch with us. There are various ways to do so:","title":"Home"},{"location":"index.html#why-should-i-use-this","text":"Imagine you integrate your Camunda Engine into a larger application landscape. In doing so the inter-system communication becomes important and questions on communication styles and patterns arise. In the world of self-contained systems, the asynchronous communication with messages is wide adopted. This library helps you to solve integration problems around correlation of messages with processes.","title":"Why should I use this?"},{"location":"index.html#how-to-start","text":"A good starting point is the Getting Started section, but then go on through the and have a look Introduction and check out our Working Examples . If you need more details on usage and configuration, check the User Guide .","title":"How to start?"},{"location":"index.html#get-in-touch","text":"If you are missing a feature, have a question regarding usage or deployment, you should definitely get in touch with us. There are various ways to do so:","title":"Get in touch"},{"location":"getting-started.html","text":"Install Dependency # First install the extension using the corresponding ingres adapter (in this example we use Kafka): <properties> <camunda-bpm-correlate.version> 0.0.1 </camunda-bpm-correlate.version> </properties> <dependencies> <dependency> <groupId> io.holunda </groupId> <artifactId> camunda-bpm-correlate-spring-boot-starter </artifactId> <version> ${camunda-bpm-correlate.version} </version> </dependency> <dependency> <groupId> io.holunda </groupId> <artifactId> camunda-bpm-correlate-spring-cloud-stream </artifactId> <version> ${camunda-bpm-correlate.version} </version> </dependency> <dependency> <groupId> org.springframework.cloud </groupId> <artifactId> spring-cloud-stream-binder-kafka </artifactId> </dependency> </dependencies> Configuration # Configure your basic Spring Cloud Streams Kafka configuration to looks like this (or similar). Important is the name of the function definition. spring : cloud : stream : function : definition : streamByteMessageConsumer bindings : streamByteMessageConsumer-in-0 : correlate-ingres-binding bindings : correlate-ingres-binding : content-type : application/json destination : ${KAFKA_TOPIC_CORRELATE_INGRES:correlate-ingres} binder : correlate-ingres-binder group : ${KAFKA_GROUP_ID} binders : correlate-ingres-binder : type : kafka defaultCandidate : false inheritEnvironment : false environment : spring : kafka : consumer : key-deserializer : org.apache.kafka.common.serialization.ByteArrayDeserializer value-deserializer : org.apache.kafka.common.serialization.ByteArrayDeserializer cloud : stream : kafka : binder : autoCreateTopics : false autoAddPartitions : false brokers : ${KAFKA_BOOTSTRAP_SERVER_HOST:localhost}:${KAFKA_BOOTSTRAP_SERVER_PORT:9092} configuration : security.protocol : ${KAFKA_SECURITY_PROTOCOL_OVERRIDE:PLAINTEXT} In addition, add the configuration of the extension: correlate : enabled : true channels : stream : channelEnabled : true message : timeToLiveAsString : PT10S # errors during TTL seconds after receiving are ignored payloadEncoding : jackson # our bytes are actually JSON written by Jackson. batch : mode : all # default fail_first -> 'all' will correlate one message after another, resulting in ignoring the order of receiving query : # query scheduler pollInitialDelay : PT10S pollInterval : PT6S cleanup : # cleanup of expired messages pollInitialDelay : PT1M pollInterval : PT1M persistence : messageMaxRetries : 5 # default 100 -> will try to deliver 5 times at most messageFetchPageSize : 100 # default 100 retry : retryMaxBackoffMinutes : 5 # default 180 -> maximum 5 minutes between retries retryBackoffBase : 2.0 # value in minutes default 2.0 -> base in the power of retry to calculate the next retry","title":"Getting started"},{"location":"getting-started.html#install-dependency","text":"First install the extension using the corresponding ingres adapter (in this example we use Kafka): <properties> <camunda-bpm-correlate.version> 0.0.1 </camunda-bpm-correlate.version> </properties> <dependencies> <dependency> <groupId> io.holunda </groupId> <artifactId> camunda-bpm-correlate-spring-boot-starter </artifactId> <version> ${camunda-bpm-correlate.version} </version> </dependency> <dependency> <groupId> io.holunda </groupId> <artifactId> camunda-bpm-correlate-spring-cloud-stream </artifactId> <version> ${camunda-bpm-correlate.version} </version> </dependency> <dependency> <groupId> org.springframework.cloud </groupId> <artifactId> spring-cloud-stream-binder-kafka </artifactId> </dependency> </dependencies>","title":"Install Dependency"},{"location":"getting-started.html#configuration","text":"Configure your basic Spring Cloud Streams Kafka configuration to looks like this (or similar). Important is the name of the function definition. spring : cloud : stream : function : definition : streamByteMessageConsumer bindings : streamByteMessageConsumer-in-0 : correlate-ingres-binding bindings : correlate-ingres-binding : content-type : application/json destination : ${KAFKA_TOPIC_CORRELATE_INGRES:correlate-ingres} binder : correlate-ingres-binder group : ${KAFKA_GROUP_ID} binders : correlate-ingres-binder : type : kafka defaultCandidate : false inheritEnvironment : false environment : spring : kafka : consumer : key-deserializer : org.apache.kafka.common.serialization.ByteArrayDeserializer value-deserializer : org.apache.kafka.common.serialization.ByteArrayDeserializer cloud : stream : kafka : binder : autoCreateTopics : false autoAddPartitions : false brokers : ${KAFKA_BOOTSTRAP_SERVER_HOST:localhost}:${KAFKA_BOOTSTRAP_SERVER_PORT:9092} configuration : security.protocol : ${KAFKA_SECURITY_PROTOCOL_OVERRIDE:PLAINTEXT} In addition, add the configuration of the extension: correlate : enabled : true channels : stream : channelEnabled : true message : timeToLiveAsString : PT10S # errors during TTL seconds after receiving are ignored payloadEncoding : jackson # our bytes are actually JSON written by Jackson. batch : mode : all # default fail_first -> 'all' will correlate one message after another, resulting in ignoring the order of receiving query : # query scheduler pollInitialDelay : PT10S pollInterval : PT6S cleanup : # cleanup of expired messages pollInitialDelay : PT1M pollInterval : PT1M persistence : messageMaxRetries : 5 # default 100 -> will try to deliver 5 times at most messageFetchPageSize : 100 # default 100 retry : retryMaxBackoffMinutes : 5 # default 180 -> maximum 5 minutes between retries retryBackoffBase : 2.0 # value in minutes default 2.0 -> base in the power of retry to calculate the next retry","title":"Configuration"},{"location":"developer-guide/contribution.html","text":"There are several ways in which you may contribute to this project. File issues Submit a pull requests Found a bug or missing feature? # Please file an issue in our issue tracking system. Submit a Pull Request # If you found a solution to an open issue and implemented it, we would be happy to add your contribution in the code base. For doing so, please create a pull request. Prior to that, please make sure you rebased against the develop branch stick to project coding conventions added test cases for the problem you are solving added docs, describing the change generally comply with codeacy report","title":"Contributing"},{"location":"developer-guide/contribution.html#found-a-bug-or-missing-feature","text":"Please file an issue in our issue tracking system.","title":"Found a bug or missing feature?"},{"location":"developer-guide/contribution.html#submit-a-pull-request","text":"If you found a solution to an open issue and implemented it, we would be happy to add your contribution in the code base. For doing so, please create a pull request. Prior to that, please make sure you rebased against the develop branch stick to project coding conventions added test cases for the problem you are solving added docs, describing the change generally comply with codeacy report","title":"Submit a Pull Request"},{"location":"developer-guide/project-setup.html","text":"If you are interested in developing and building the project please read the following the instructions carefully. Version control # To get sources of the project, please execute: git clone https://github.com/holunda-io/camunda-bpm-correlate.git cd camunda-rest-client-spring-boot We are using gitflow in our git SCM for naming branches. That means that you should start from develop branch, create a feature/<name> out of it and once it is completed create a pull request containing it. Please squash your commits before submitting and use semantic commit messages, if possible. Project Build # Perform the following steps to get a development setup up and running. ./mvnw clean install Integration Tests # By default, the build command will ignore the run of failsafe Maven plugin executing the integration tests (usual JUnit tests with class names ending with ITest). In order to run integration tests, please call from your command line: ./mvnw -Pitest Project build modes and profiles # Documentation # We are using MkDocs for generation of a static site documentation and rely on markdown as much as possible. Note If you want to develop your docs in 'live' mode, run mkdocs serve and access the http://localhost:8000/ from your browser. For creation of documentation, please run: Generation of JavaDoc and Sources # By default, the sources and javadoc API documentation are not generated from the source code. To enable this: ./mvnw clean install -Prelease -Dgpg.skip = true Continuous Integration # GitHub Actions are building all branches on commit hook (for codecov). In addition, a GitHub Actions are used to build PRs and all branches. Release Management # The release is produced by using the GitHub feature \"Close Milestone\". A special GitHub action is preparing the release notes as a draft. Then click on \"Publish Release\" to make it public. What modules get deployed to repository # Every Maven module is enabled by default. If you want to change this, please provide the property <maven.deploy.skip> true </maven.deploy.skip> inside the corresponding pom.xml . Currently, all examples are EXCLUDED from publication into Maven Central.","title":"Project Setup"},{"location":"developer-guide/project-setup.html#version-control","text":"To get sources of the project, please execute: git clone https://github.com/holunda-io/camunda-bpm-correlate.git cd camunda-rest-client-spring-boot We are using gitflow in our git SCM for naming branches. That means that you should start from develop branch, create a feature/<name> out of it and once it is completed create a pull request containing it. Please squash your commits before submitting and use semantic commit messages, if possible.","title":"Version control"},{"location":"developer-guide/project-setup.html#project-build","text":"Perform the following steps to get a development setup up and running. ./mvnw clean install","title":"Project Build"},{"location":"developer-guide/project-setup.html#integration-tests","text":"By default, the build command will ignore the run of failsafe Maven plugin executing the integration tests (usual JUnit tests with class names ending with ITest). In order to run integration tests, please call from your command line: ./mvnw -Pitest","title":"Integration Tests"},{"location":"developer-guide/project-setup.html#project-build-modes-and-profiles","text":"","title":"Project build modes and profiles"},{"location":"developer-guide/project-setup.html#documentation","text":"We are using MkDocs for generation of a static site documentation and rely on markdown as much as possible. Note If you want to develop your docs in 'live' mode, run mkdocs serve and access the http://localhost:8000/ from your browser. For creation of documentation, please run:","title":"Documentation"},{"location":"developer-guide/project-setup.html#generation-of-javadoc-and-sources","text":"By default, the sources and javadoc API documentation are not generated from the source code. To enable this: ./mvnw clean install -Prelease -Dgpg.skip = true","title":"Generation of JavaDoc and Sources"},{"location":"developer-guide/project-setup.html#continuous-integration","text":"GitHub Actions are building all branches on commit hook (for codecov). In addition, a GitHub Actions are used to build PRs and all branches.","title":"Continuous Integration"},{"location":"developer-guide/project-setup.html#release-management","text":"The release is produced by using the GitHub feature \"Close Milestone\". A special GitHub action is preparing the release notes as a draft. Then click on \"Publish Release\" to make it public.","title":"Release Management"},{"location":"developer-guide/project-setup.html#what-modules-get-deployed-to-repository","text":"Every Maven module is enabled by default. If you want to change this, please provide the property <maven.deploy.skip> true </maven.deploy.skip> inside the corresponding pom.xml . Currently, all examples are EXCLUDED from publication into Maven Central.","title":"What modules get deployed to repository"},{"location":"introduction/index.html","text":"If you are visiting this project for the first time, please check the following sections: Motivation Features Solution Strategy Further Outlook","title":"Start here"},{"location":"introduction/features.html","text":"Currently, the library supports the following features: General # Ingres Adapters: Spring Cloud Kafka Axon Framework MetaData extractors: Message based (Headers) Channel based (Properties) Persisting Message Accepting Adapter Message Persistence In-Memory MyBatis (using the same DB as Camunda Platform 7) Batch processor running on schedule Batch modes: all, fail-first Correlation error strategies: ignore, drop, retry Message Buffering (TTL) Message Expiry","title":"Features"},{"location":"introduction/features.html#general","text":"Ingres Adapters: Spring Cloud Kafka Axon Framework MetaData extractors: Message based (Headers) Channel based (Properties) Persisting Message Accepting Adapter Message Persistence In-Memory MyBatis (using the same DB as Camunda Platform 7) Batch processor running on schedule Batch modes: all, fail-first Correlation error strategies: ignore, drop, retry Message Buffering (TTL) Message Expiry","title":"General"},{"location":"introduction/further-outlook.html","text":"Other ideas # Plugin for Camunda Cockpit More metrics for Prometheus More ingres adapters: Apache Camel REST implementing Camunda API Is the library missing a feature important for you? Please report it .","title":"Further Outlook"},{"location":"introduction/further-outlook.html#other-ideas","text":"Plugin for Camunda Cockpit More metrics for Prometheus More ingres adapters: Apache Camel REST implementing Camunda API Is the library missing a feature important for you? Please report it .","title":"Other ideas"},{"location":"introduction/motivation.html","text":"Correlation is about targeting a running workflow (for example running inside the Camunda Platform 7) containing the state update by an external system. Inside the Camunda Platform it is important that the message subscription is present at the time of correlation, otherwise the correlation is mismatched. If you are building a distributed system using the Camunda Platform 7 as a part of it, you should not make assumptions or assertions regarding the speed of processing of components, message ordering, message delivery or timings. To make sure that the correlation is not dependent on all those assumptions, the usage of inbox pattern to store the message locally and then deliver it timely on schedule is a good practise.","title":"Motivation"},{"location":"introduction/solution.html","text":"The library provides a core that is responsible for accepting the message, storing it into persistence storage and processing it scheduled. If any errors occur during the correlation, these are handled by one of the pre-configured error strategies, like retry, ignore or drop... In addition, there are a set of several ingres adapters to support different communication technologies.","title":"Solution"},{"location":"user-guide/index.html","text":"The user guide consists of several sections. Concepts # Architecture Ingres Adapter Axon Framework Spring Cloud Stream Message Acceptor Message Persistence Examples # Kafka Example Axon Example References # Camunda Community Summit 2022 Talk","title":"Overview"},{"location":"user-guide/index.html#concepts","text":"Architecture Ingres Adapter Axon Framework Spring Cloud Stream Message Acceptor Message Persistence","title":"Concepts"},{"location":"user-guide/index.html#examples","text":"Kafka Example Axon Example","title":"Examples"},{"location":"user-guide/index.html#references","text":"Camunda Community Summit 2022 Talk","title":"References"},{"location":"user-guide/architecture.html","text":"","title":"Architecture"},{"location":"user-guide/examples.html","text":"Working example # We provide examples demonstrating the usage of the library with different messaging technologies. The general example is a \"Travel Reservation\" business process, aiming to book flights and a hotel in a target city. The reservation process receives the customer name, the source city, the target city and the dates of the travel. Using this information, it requests the flights by the flight service and a hotel by the hotel service. The confirmed flight information and hotel information is stored inside the process payload variables. The following sequence diagram illustrates the timing of messages being passed: As a result, the HotelReservationConfirmed is received before FlightReservationConfirmed causing a trouble, if message order is strict and must be preserved. In addition, at the time of the receiving of the first message, the process has not completed the long-running task of saving the details. In the following examples we are not focusing on demonstration of features resulting from the race conditions (resolved faulty), but on purpose created illegal (unwished) message ordering, causing to fail the orchestration by design. Software requirements to run examples # Docker Docker-Compose KCat (formerly known as KafkaCat) JQ Curl Spring Cloud Streams with Kafka # The example demonstrates the usage of the library using Kafka for communication. In doing so, we rely on the Spring Cloud Streams binding for Kafka. We constructed an example sending and receiving data between services using Apache Kafka. To run the examples, you will need to have Docker installed on your machine. Please first run the build of the examples and construct the container images...To do so, please run: mvn clean install -Pdocker-assembly -f example Then start the provided images using the supplied docker-compose file, by running: docker-compose -f example/spring-cloud/docker-compose.yml up -d This command will start Apache Kafka, Zookeeper, Flight Service and Hotel Service locally. As a next step, open your IDE and run the io/holunda/camunda/bpm/example/kafka/TravelAgencyKafkaCorrelationApplication.kt application by providing the spring profile camunda-correlate (module example/spring-cloud/reservation-kafka ). Having it all up-and running, you can send the first message, by using the provided script, which uses kcat / kafkacat and jq . Please run: example/spring-cloud/example.sh reservation to send the message to the reservation topic. As a result, the process should get started, and you should see the messages [SEND BOOK FLIGHT] and [SEND BOOK HOTEL] in your log, indicating that the messages are sent to corresponding topics. The services are executed delayed (2 secs, 5 secs), during the process is executing the long-running task (saving the reservation details lasts 10 seconds), you will see the response messages coming in. For demonstration purposes, the service delays are configured in a way that the \"expected\" answer by the flight is received after the \"unexpected\" response from the hotel is received. Therefor, you will see the exception ( MismatchedCorrelationException ) in the log first. After this, you can inspect the content of the inbox by calling an endpoint http://localhost:8080/admin/list-messages/ . Axon Events aka using Camunda Platform 7 as Microservice Orchestrator # The example demonstrates the usage of the library using Axon Framework / Axon Server based communication. In doing so, We constructed an example sending and receiving data between services using Axon Command and Event Buses. To run the examples, you will need to have Docker installed on your machine. Please first run the build of the examples and construct the container images...To do so, please run: mvn clean install -Pdocker-assembly -f example Then start the provided images using the supplied docker-compose file, by running: docker-compose -f example/axon/docker-compose.yml up -d This command will start Axon Server, Flight Service and Hotel Service locally. As a next step, open your IDE and run the io/holunda/camunda/bpm/example/axon/TravelAgencyAxonCorrelationApplication.kt application (module example/axon/reservation-axon ). Having it all up-and running, you can send the first message, by using the provided script, which uses curl and jq . Please run: example/axon/example.sh reservation to send the message to the REST-ful endpoint, which will emit an Axon event. As a result, the process should get started, and you should see the messages [SEND BOOK FLIGHT] and [SEND BOOK HOTEL] in your log, indicating that the commands are sent to. The services are executed delayed (2 secs, 5 secs), during the process is executing the long-running task (saving the reservation details lasts 10 seconds), you will see the response messages coming in. For demonstration purposes, the service delays are configured in a way that the \"expected\" answer by the flight is received after the \"unexpected\" response from the hotel is received. Therefor, you will see the exception ( MismatchedCorrelationException ) in the log first. After this, you can inspect the content of the inbox by calling an endpoint http://localhost:8080/admin/list-messages/ .","title":"Examples"},{"location":"user-guide/examples.html#working-example","text":"We provide examples demonstrating the usage of the library with different messaging technologies. The general example is a \"Travel Reservation\" business process, aiming to book flights and a hotel in a target city. The reservation process receives the customer name, the source city, the target city and the dates of the travel. Using this information, it requests the flights by the flight service and a hotel by the hotel service. The confirmed flight information and hotel information is stored inside the process payload variables. The following sequence diagram illustrates the timing of messages being passed: As a result, the HotelReservationConfirmed is received before FlightReservationConfirmed causing a trouble, if message order is strict and must be preserved. In addition, at the time of the receiving of the first message, the process has not completed the long-running task of saving the details. In the following examples we are not focusing on demonstration of features resulting from the race conditions (resolved faulty), but on purpose created illegal (unwished) message ordering, causing to fail the orchestration by design.","title":"Working example"},{"location":"user-guide/examples.html#software-requirements-to-run-examples","text":"Docker Docker-Compose KCat (formerly known as KafkaCat) JQ Curl","title":"Software requirements to run examples"},{"location":"user-guide/examples.html#spring-cloud-streams-with-kafka","text":"The example demonstrates the usage of the library using Kafka for communication. In doing so, we rely on the Spring Cloud Streams binding for Kafka. We constructed an example sending and receiving data between services using Apache Kafka. To run the examples, you will need to have Docker installed on your machine. Please first run the build of the examples and construct the container images...To do so, please run: mvn clean install -Pdocker-assembly -f example Then start the provided images using the supplied docker-compose file, by running: docker-compose -f example/spring-cloud/docker-compose.yml up -d This command will start Apache Kafka, Zookeeper, Flight Service and Hotel Service locally. As a next step, open your IDE and run the io/holunda/camunda/bpm/example/kafka/TravelAgencyKafkaCorrelationApplication.kt application by providing the spring profile camunda-correlate (module example/spring-cloud/reservation-kafka ). Having it all up-and running, you can send the first message, by using the provided script, which uses kcat / kafkacat and jq . Please run: example/spring-cloud/example.sh reservation to send the message to the reservation topic. As a result, the process should get started, and you should see the messages [SEND BOOK FLIGHT] and [SEND BOOK HOTEL] in your log, indicating that the messages are sent to corresponding topics. The services are executed delayed (2 secs, 5 secs), during the process is executing the long-running task (saving the reservation details lasts 10 seconds), you will see the response messages coming in. For demonstration purposes, the service delays are configured in a way that the \"expected\" answer by the flight is received after the \"unexpected\" response from the hotel is received. Therefor, you will see the exception ( MismatchedCorrelationException ) in the log first. After this, you can inspect the content of the inbox by calling an endpoint http://localhost:8080/admin/list-messages/ .","title":"Spring Cloud Streams with Kafka"},{"location":"user-guide/examples.html#axon-events-aka-using-camunda-platform-7-as-microservice-orchestrator","text":"The example demonstrates the usage of the library using Axon Framework / Axon Server based communication. In doing so, We constructed an example sending and receiving data between services using Axon Command and Event Buses. To run the examples, you will need to have Docker installed on your machine. Please first run the build of the examples and construct the container images...To do so, please run: mvn clean install -Pdocker-assembly -f example Then start the provided images using the supplied docker-compose file, by running: docker-compose -f example/axon/docker-compose.yml up -d This command will start Axon Server, Flight Service and Hotel Service locally. As a next step, open your IDE and run the io/holunda/camunda/bpm/example/axon/TravelAgencyAxonCorrelationApplication.kt application (module example/axon/reservation-axon ). Having it all up-and running, you can send the first message, by using the provided script, which uses curl and jq . Please run: example/axon/example.sh reservation to send the message to the REST-ful endpoint, which will emit an Axon event. As a result, the process should get started, and you should see the messages [SEND BOOK FLIGHT] and [SEND BOOK HOTEL] in your log, indicating that the commands are sent to. The services are executed delayed (2 secs, 5 secs), during the process is executing the long-running task (saving the reservation details lasts 10 seconds), you will see the response messages coming in. For demonstration purposes, the service delays are configured in a way that the \"expected\" answer by the flight is received after the \"unexpected\" response from the hotel is received. Therefor, you will see the exception ( MismatchedCorrelationException ) in the log first. After this, you can inspect the content of the inbox by calling an endpoint http://localhost:8080/admin/list-messages/ .","title":"Axon Events aka using Camunda Platform 7 as Microservice Orchestrator"},{"location":"user-guide/filtering-messages.html","text":"In messaging scenarios it is not uncommon that the message bus is transporting more types of message than the current system should consume. In this case, it is important to filter out and ignore the irrelevant messages and take only the relevant messages into consideration. In the same time, all messages still needs to be consumed in order not to block the follow-up messages. In order to filter the messages between the Ingres Adapter and the Message Acceptor we supply a special MessageFilter to filter out the supported messaged. /** * Message filter to filter messages delivered to the message acceptor. */ interface MessageFilter { /** * Checks if the message should be delivered to the message acceptor. * @param message message instance * @param messageMetaData metadata of the message * @return `true` if message should be accepted. */ fun < P > accepts ( message : AbstractChannelMessage < P > , messageMetaData : MessageMetaData ): Boolean } Feel free to implement your own filters and supply them as a Spring Bean or choose on of the predefined filter and use them in your setup. AllMessageFilter # The AllMessageFilter accepts all messages received from the Ingres adapter. AndCompositeMessageFilter # The AndCompositeMessageFilter is a composite filter consisting of a list of MessageFilter implementations combined by a logical AND operator. OrCompositeMessageFilter # The OrCompositeMessageFilter is a composite filter consisting of a list of MessageFilter implementations combined by a logical OR operator. NotMessageFilter # The NotMessageFilter is a filter inverting the application of a supplied MessageFilter implementation. TypeListMessageFilter # The TypeListMessageFilter is a filter accepting all messages with a payload type being one of the specified types. TypeExistsOnClasspathMessageFilter # The TypeExistsOnClasspathMessageFilter is accepting all messages that payload type is available on the class path.","title":"Message Filter"},{"location":"user-guide/filtering-messages.html#allmessagefilter","text":"The AllMessageFilter accepts all messages received from the Ingres adapter.","title":"AllMessageFilter"},{"location":"user-guide/filtering-messages.html#andcompositemessagefilter","text":"The AndCompositeMessageFilter is a composite filter consisting of a list of MessageFilter implementations combined by a logical AND operator.","title":"AndCompositeMessageFilter"},{"location":"user-guide/filtering-messages.html#orcompositemessagefilter","text":"The OrCompositeMessageFilter is a composite filter consisting of a list of MessageFilter implementations combined by a logical OR operator.","title":"OrCompositeMessageFilter"},{"location":"user-guide/filtering-messages.html#notmessagefilter","text":"The NotMessageFilter is a filter inverting the application of a supplied MessageFilter implementation.","title":"NotMessageFilter"},{"location":"user-guide/filtering-messages.html#typelistmessagefilter","text":"The TypeListMessageFilter is a filter accepting all messages with a payload type being one of the specified types.","title":"TypeListMessageFilter"},{"location":"user-guide/filtering-messages.html#typeexistsonclasspathmessagefilter","text":"The TypeExistsOnClasspathMessageFilter is accepting all messages that payload type is available on the class path.","title":"TypeExistsOnClasspathMessageFilter"},{"location":"user-guide/ingres-axon.html","text":"The Axon Framework Ingres Adapter is responsible for receiving events on Axon Event bus and sending them for the correlation to the library. Message # Axon Event Message is received and deserialized by Axon Framework, using the configured message de-serializer and passed to the ingres adapter. The adapter is reading headers from message MetaData and converts them into message headers. The payload is encoded into serializable payload using the configured encoder (currently Jackson).","title":"Axon Framework Ingres Adapter"},{"location":"user-guide/ingres-axon.html#message","text":"Axon Event Message is received and deserialized by Axon Framework, using the configured message de-serializer and passed to the ingres adapter. The adapter is reading headers from message MetaData and converts them into message headers. The payload is encoded into serializable payload using the configured encoder (currently Jackson).","title":"Message"},{"location":"user-guide/ingres-spring-cloud.html","text":"The Spring Cloud Ingres Adapter is a component responsible for receiving Spring Cloud messages (using configured binding like Kafka, AMQP or others) and convert them into message format used by the library. Message # Kafka Message is received and the Kafka headers are converted to message headers.","title":"Spring Cloud Ingres Adapter"},{"location":"user-guide/ingres-spring-cloud.html#message","text":"Kafka Message is received and the Kafka headers are converted to message headers.","title":"Message"},{"location":"user-guide/ingres.html","text":"The ingres adapter is a component responsible for the adaptation of the communication technology to the uniform message format used in the library. It is responsible for creation the instance of a message including message headers and serialized message payload.","title":"Ingres Adapter"},{"location":"user-guide/message-acceptor.html","text":"The message acceptor is a component responsible to receive the message from the Ingres adapter and store it into the database. Message Metadata # Along with the payload the message must contain metadata represented by the MessageMetadata . This metadata is extracted from the channel settings, message payload and other sources. Usually the MessageMetadata instance can't be constructed at one place, so we supply the MessageMetaDataExtractorChain consisting of MessageMetaDataSnippetExtractor instances. By doing so, every aspect of metadata extraction is put in it own class and the result of the extraction is gathered in one MessageMetadata instance. To simplify the construction of MessageMetaDataExtractorChain we supply several MessageMetaDataSnippetExtractor implementations: ChannelConfigMessageMetaDataSnippetExtractor # The ChannelConfigMessageMetaDataSnippetExtractor is responsible for reading metadata from the channel configuration. Usually, channel configuration parameters like message encoding are extracted that way. HeaderMessageMessageMetaDataSnippetExtractor # The HeaderMessageMessageMetaDataSnippetExtractor is responsible for reading metadata from message headers. Most communication technologies support some concept of message headers and the corresponding Ingres Adapter is mapping those headers to message headers used in the library. By doing so, you can influence message attributes on a message level. For example, if you are receiving different types of messages by the same ingres adapter, this is the easiest way to detect the type of the message. Message filtering # Sometimes, more messages are sent through the channel as needed to be received by the Ingres Adapter and needs to be accepted and stored by the library. For this purpose, a special filtering component MessageFilter is configured in the MessageAcceptor . For more details, how to use the filter please check the Filtering messages section. You can configure what messages it will support and filter the messages that will be delivered to the message acceptor. Persisting Channel Message Acceptor # To implement the inbox pattern, the message acceptor stores received messages in a persistent storage. For this purpose, the PersistingChannelMessageAcceptor is implemented. For more details, please check the Message Persistence section.","title":"Message Acceptor"},{"location":"user-guide/message-acceptor.html#message-metadata","text":"Along with the payload the message must contain metadata represented by the MessageMetadata . This metadata is extracted from the channel settings, message payload and other sources. Usually the MessageMetadata instance can't be constructed at one place, so we supply the MessageMetaDataExtractorChain consisting of MessageMetaDataSnippetExtractor instances. By doing so, every aspect of metadata extraction is put in it own class and the result of the extraction is gathered in one MessageMetadata instance. To simplify the construction of MessageMetaDataExtractorChain we supply several MessageMetaDataSnippetExtractor implementations:","title":"Message Metadata"},{"location":"user-guide/message-acceptor.html#channelconfigmessagemetadatasnippetextractor","text":"The ChannelConfigMessageMetaDataSnippetExtractor is responsible for reading metadata from the channel configuration. Usually, channel configuration parameters like message encoding are extracted that way.","title":"ChannelConfigMessageMetaDataSnippetExtractor"},{"location":"user-guide/message-acceptor.html#headermessagemessagemetadatasnippetextractor","text":"The HeaderMessageMessageMetaDataSnippetExtractor is responsible for reading metadata from message headers. Most communication technologies support some concept of message headers and the corresponding Ingres Adapter is mapping those headers to message headers used in the library. By doing so, you can influence message attributes on a message level. For example, if you are receiving different types of messages by the same ingres adapter, this is the easiest way to detect the type of the message.","title":"HeaderMessageMessageMetaDataSnippetExtractor"},{"location":"user-guide/message-acceptor.html#message-filtering","text":"Sometimes, more messages are sent through the channel as needed to be received by the Ingres Adapter and needs to be accepted and stored by the library. For this purpose, a special filtering component MessageFilter is configured in the MessageAcceptor . For more details, how to use the filter please check the Filtering messages section. You can configure what messages it will support and filter the messages that will be delivered to the message acceptor.","title":"Message filtering"},{"location":"user-guide/message-acceptor.html#persisting-channel-message-acceptor","text":"To implement the inbox pattern, the message acceptor stores received messages in a persistent storage. For this purpose, the PersistingChannelMessageAcceptor is implemented. For more details, please check the Message Persistence section.","title":"Persisting Channel Message Acceptor"},{"location":"user-guide/message-persistence.html","text":"The messages received and accepted by the message acceptor are stored in a relation database. For doing so, we implemented a MyBatis mapper of the underlying entity in order to keep the dependency track as small as possible (MyBatis is a library used and supplied by Camunda Platform 7). For the persistence of the messages the library uses a database table COR_MESSAGE with the following structure: Column Java Datatype JDBC Datatype Description ID String VARCHAR Message id (unique) PAYLOAD_ENCODING String VARCHAR Encoding of the payload PAYLOAD_TYPE_NAMESPACE String VARCHAR Namespace of the payload type, for example package PAYLOAD_TYPE_NAME String VARCHAR Simple type name of the payload type, for example class name PAYLOAD_TYPE_REVISION String VARCHAR Revision of the payload type. PAYLOAD ByteArray BINARY Byte array containing the encoded payload INSERTED Instant TIMESTAMP WITH TIMEZONE Timestamp of message ingestion TTL_DURATION String VARCHAR Time to live of the message as Duration string EXPIRATION Instant TIMESTAMP WITH TIMEZONE Expiration of the message as timestamp RETRIES Integer INTEGER Number of retries of message correlation NEXT_RETRY Instant TIMESTAMP WITH TIMEZONE Timestamp of the next retry ERROR String VARCHAR Last error stacktrace produced during correlation Depending on your database you will need different SQL DDLs to create the underlying DB table. Here are some dialects, we already tried out: MS SQL / Azure SQL # We use NVARCHAR as a basic type for strings because of improved index performance for UTF-8 encoded strings. CREATE TABLE COR_MESSAGE ( ID NVARCHAR ( 64 ) UNIQUE NOT NULL , PAYLOAD_ENCODING NVARCHAR ( 64 ) NOT NULL , PAYLOAD_TYPE_NAMESPACE NVARCHAR ( 128 ) NOT NULL , PAYLOAD_TYPE_NAME NVARCHAR ( 128 ) NOT NULL , PAYLOAD_TYPE_REVISION NVARCHAR ( 64 ), PAYLOAD BINARY ( 4096 ), INSERTED DATETIME2 NOT NULL , TTL_DURATION NVARCHAR ( 32 ), EXPIRATION DATETIME2 , RETRIES INT NOT NULL , NEXT_RETRY DATETIME2 , ERROR NVARCHAR ( MAX ) ); H2 / HSQL # CREATE TABLE COR_MESSAGE ( ID VARCHAR2(64) UNIQUE NOT NULL, PAYLOAD_ENCODING VARCHAR2(64) NOT NULL, PAYLOAD_TYPE_NAMESPACE VARCHAR2(128) NOT NULL, PAYLOAD_TYPE_NAME VARCHAR2(128) NOT NULL, PAYLOAD_TYPE_REVISION VARCHAR2(64), PAYLOAD BINARY(4096), INSERTED TIMESTAMP NOT NULL, TTL_DURATION VARCHAR2(32), EXPIRATION TIMESTAMP, RETRIES INTEGER NOT NULL, NEXT_RETRY TIMESTAMP, ERROR CLOB(10000) );","title":"Message Persistence"},{"location":"user-guide/message-persistence.html#ms-sql-azure-sql","text":"We use NVARCHAR as a basic type for strings because of improved index performance for UTF-8 encoded strings. CREATE TABLE COR_MESSAGE ( ID NVARCHAR ( 64 ) UNIQUE NOT NULL , PAYLOAD_ENCODING NVARCHAR ( 64 ) NOT NULL , PAYLOAD_TYPE_NAMESPACE NVARCHAR ( 128 ) NOT NULL , PAYLOAD_TYPE_NAME NVARCHAR ( 128 ) NOT NULL , PAYLOAD_TYPE_REVISION NVARCHAR ( 64 ), PAYLOAD BINARY ( 4096 ), INSERTED DATETIME2 NOT NULL , TTL_DURATION NVARCHAR ( 32 ), EXPIRATION DATETIME2 , RETRIES INT NOT NULL , NEXT_RETRY DATETIME2 , ERROR NVARCHAR ( MAX ) );","title":"MS SQL / Azure SQL"},{"location":"user-guide/message-persistence.html#h2-hsql","text":"CREATE TABLE COR_MESSAGE ( ID VARCHAR2(64) UNIQUE NOT NULL, PAYLOAD_ENCODING VARCHAR2(64) NOT NULL, PAYLOAD_TYPE_NAMESPACE VARCHAR2(128) NOT NULL, PAYLOAD_TYPE_NAME VARCHAR2(128) NOT NULL, PAYLOAD_TYPE_REVISION VARCHAR2(64), PAYLOAD BINARY(4096), INSERTED TIMESTAMP NOT NULL, TTL_DURATION VARCHAR2(32), EXPIRATION TIMESTAMP, RETRIES INTEGER NOT NULL, NEXT_RETRY TIMESTAMP, ERROR CLOB(10000) );","title":"H2 / HSQL"},{"location":"user-guide/message-processing.html","text":"Messages are processed in batches triggered by a scheduler. There are following options to set-up the attributes of this schduler: ... More details will follow Running in a cluster # For a cluster operations it is important to synchronize the batch schedulers between the cluster nodes. For this purpose, the library Shedlock is used. Shedlock synchronizes the scheduled tasks using a RDBMS table. Here are the required DDL snippets for some common RDBMSs. CREATE TABLE shedlock ( name NVARCHAR ( 64 ) NOT NULL , lock_until DATETIME2 NOT NULL , locked_at DATETIME2 NOT NULL , locked_by NVARCHAR ( 255 ) NOT NULL , PRIMARY KEY ( name ) ); CREATE TABLE shedlock ( name VARCHAR(64) NOT NULL, lock_until DATETIME2 NOT NULL, locked_at DATETIME2 NOT NULL, locked_by VARCHAR(255) NOT NULL, PRIMARY KEY (name) );","title":"Message processing"},{"location":"user-guide/message-processing.html#running-in-a-cluster","text":"For a cluster operations it is important to synchronize the batch schedulers between the cluster nodes. For this purpose, the library Shedlock is used. Shedlock synchronizes the scheduled tasks using a RDBMS table. Here are the required DDL snippets for some common RDBMSs. CREATE TABLE shedlock ( name NVARCHAR ( 64 ) NOT NULL , lock_until DATETIME2 NOT NULL , locked_at DATETIME2 NOT NULL , locked_by NVARCHAR ( 255 ) NOT NULL , PRIMARY KEY ( name ) ); CREATE TABLE shedlock ( name VARCHAR(64) NOT NULL, lock_until DATETIME2 NOT NULL, locked_at DATETIME2 NOT NULL, locked_by VARCHAR(255) NOT NULL, PRIMARY KEY (name) );","title":"Running in a cluster"}]}