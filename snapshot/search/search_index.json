{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#why-should-i-use-this","title":"Why should I use this?","text":"<p>Imagine you integrate your Camunda Engine into a larger application landscape. In doing so the inter-system communication becomes important and questions on communication styles and patterns arise. In the world of self-contained systems, the asynchronous communication with messages is wide adopted. This library helps you to solve integration problems around correlation of messages with processes.</p>"},{"location":"index.html#how-to-start","title":"How to start?","text":"<p>A good starting point is the Getting Started section, but then go on through the  and have a look Introduction and check out our Working Examples. If you need more details on usage and configuration, check the User Guide.</p>"},{"location":"index.html#get-in-touch","title":"Get in touch","text":"<p>If you are missing a feature, have a question regarding usage or deployment, you should definitely get in touch with us. There are various ways to do so:</p> <p> </p>"},{"location":"getting-started.html","title":"Getting started","text":""},{"location":"getting-started.html#install-dependency","title":"Install Dependency","text":"<p>First install the extension using the corresponding ingress adapter (in this example we use Spring Cloud Stream for connecting with Kafka):</p> <pre><code>&lt;properties&gt;\n&lt;camunda-bpm-correlate.version&gt;1.0.0&lt;/camunda-bpm-correlate.version&gt;\n&lt;/properties&gt;\n\n&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;io.holunda&lt;/groupId&gt;\n&lt;artifactId&gt;camunda-bpm-correlate-spring-boot-starter&lt;/artifactId&gt;\n&lt;version&gt;${camunda-bpm-correlate.version}&lt;/version&gt;\n&lt;/dependency&gt;\n\n&lt;dependency&gt;\n&lt;groupId&gt;io.holunda&lt;/groupId&gt;\n&lt;artifactId&gt;camunda-bpm-correlate-spring-cloud-stream&lt;/artifactId&gt;\n&lt;version&gt;${camunda-bpm-correlate.version}&lt;/version&gt;\n&lt;/dependency&gt;\n\n&lt;dependency&gt;\n&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\n&lt;artifactId&gt;spring-cloud-stream-binder-kafka&lt;/artifactId&gt;\n&lt;/dependency&gt;\n\n&lt;/dependencies&gt;\n</code></pre>"},{"location":"getting-started.html#configuration","title":"Configuration","text":"<p>Please add the configuration of the extension:</p> <pre><code>correlate:\nenabled: true\nchannels:\nmy-kafka-channel:\nenabled: true\ntype: stream\nbeanName: special-name\nbatch:\nmode: all # default fail_first -&gt; 'all' will correlate one message after another, resulting in ignoring the order of receiving\nquery:    # query scheduler\npollInitialDelay: PT10S\npollInterval: PT6S\ncleanup:  # cleanup of expired messages\npollInitialDelay: PT1M\npollInterval: PT1M\nmessage:\ntimeToLiveAsString: PT10S # errors during TTL seconds after receiving are ignored\npayloadEncoding: jackson  # our bytes are actually JSON written by Jackson.\npersistence:\nmessageMaxRetries: 5 # default 100 -&gt; will try to deliver 5 times at most\nmessageFetchPageSize: 100 # default 100\nretry:\nretryMaxBackoffMinutes: 5 # default 180 -&gt; maximum 5 minutes between retries\nretryBackoffBase: 2.0 # value in minutes default 2.0 -&gt; base in the power of retry to calculate the next retry\n</code></pre> <p>Now configure your basic Spring Cloud Streams Kafka configuration to looks like this (or similar). Pay attention to the name of the function definition and the bindings' in channels. It results from the  value of <code>correlate.channels.&lt;channel-nam&gt;.beanName</code> and accordingly is part of the expression to  bind the parameter of the binding (<code>special-name-in-0</code>).</p> <pre><code>spring:\ncloud:\nstream:\nfunction:\ndefinition: special-name\nbindings:\nspecial-name-in-0: correlate-ingress-binding      bindings:\ncorrelate-ingress-binding:\ncontent-type: application/json\ndestination: ${KAFKA_TOPIC_CORRELATE_INGRES:correlate-ingress}\nbinder: correlate-ingress-binder\ngroup: ${KAFKA_GROUP_ID}\nbinders:\ncorrelate-ingress-binder:\ntype: kafka\ndefaultCandidate: false\ninheritEnvironment: false\nenvironment:\nspring:\nkafka:\nconsumer:\nkey-deserializer: org.apache.kafka.common.serialization.ByteArrayDeserializer\nvalue-deserializer: org.apache.kafka.common.serialization.ByteArrayDeserializer\ncloud:\nstream:\nkafka:\nbinder:\nautoCreateTopics: false\nautoAddPartitions: false\nbrokers: ${KAFKA_BOOTSTRAP_SERVER_HOST:localhost}:${KAFKA_BOOTSTRAP_SERVER_PORT:9092}\nconfiguration:\nsecurity.protocol: ${KAFKA_SECURITY_PROTOCOL_OVERRIDE:PLAINTEXT}\n</code></pre>"},{"location":"developer-guide/contribution.html","title":"Contributing","text":"<p>There are several ways in which you may contribute to this project.</p> <ul> <li>File issues</li> <li>Submit a pull requests</li> </ul>"},{"location":"developer-guide/contribution.html#found-a-bug-or-missing-feature","title":"Found a bug or missing feature?","text":"<p>Please file an issue in our issue tracking system.</p>"},{"location":"developer-guide/contribution.html#submit-a-pull-request","title":"Submit a Pull Request","text":"<p>If you found a solution to an open issue and implemented it, we would be happy to add your contribution in the code base. For doing so, please create a pull request. Prior to that, please make sure you</p> <ul> <li>rebased against the <code>develop</code> branch</li> <li>stick to project coding conventions</li> <li>added test cases for the problem you are solving</li> <li>added docs, describing the change</li> <li>generally comply with codeacy report</li> </ul>"},{"location":"developer-guide/project-setup.html","title":"Project Setup","text":"<p>If you are interested in developing and building the project please read the following the instructions carefully.</p>"},{"location":"developer-guide/project-setup.html#version-control","title":"Version control","text":"<p>To get sources of the project, please execute:</p> <pre><code>git clone https://github.com/holunda-io/camunda-bpm-correlate.git\ncd camunda-bpm-correlate\n</code></pre> <p>We are using gitflow in our git SCM for naming branches. That means that you should start from <code>develop</code> branch, create a <code>feature/&lt;name&gt;</code> out of it and once it is completed create a pull request containing it. Please squash your commits before submitting and use semantic commit messages, if possible.</p>"},{"location":"developer-guide/project-setup.html#project-build","title":"Project Build","text":"<p>Perform the following steps to get a development setup up and running.</p> <pre><code>./mvnw clean install\n</code></pre>"},{"location":"developer-guide/project-setup.html#integration-tests","title":"Integration Tests","text":"<p>By default, the build command will ignore the run of <code>failsafe</code> Maven plugin executing the integration tests (usual JUnit tests with class names ending with ITest). In order to run integration tests, please call from your command line:</p> <pre><code>./mvnw -Pitest\n</code></pre>"},{"location":"developer-guide/project-setup.html#project-build-modes-and-profiles","title":"Project build modes and profiles","text":""},{"location":"developer-guide/project-setup.html#documentation","title":"Documentation","text":"<p>We are using MkDocs for generation of a static site documentation and rely on markdown as much as possible.</p> <p>Note</p> <p>If you want to develop your docs in 'live' mode, run <code>mkdocs serve</code> and access the http://localhost:8000/ from your browser.</p> <p>For creation of documentation, please run:</p>"},{"location":"developer-guide/project-setup.html#generation-of-javadoc-and-sources","title":"Generation of JavaDoc and Sources","text":"<p>By default, the sources and javadoc API documentation are not generated from the source code. To enable this:</p> <pre><code>./mvnw clean install -Prelease -Dgpg.skip=true\n</code></pre>"},{"location":"developer-guide/project-setup.html#continuous-integration","title":"Continuous Integration","text":"<p>GitHub Actions are building all branches on commit hook (for codecov). In addition, a GitHub Actions are used to build PRs and all branches.</p>"},{"location":"developer-guide/project-setup.html#publish-a-new-release","title":"Publish a new release","text":"<p>We use gitflow plugin to handle versioning and branch manipulations between <code>develop</code> and <code>master</code>. Technically, every push to the <code>master</code> branch triggers the execution of the GH actions job producing a release and publishing it into Maven Central. To do it correctly (with correct versions) please run:</p> <pre><code>./mvnw gitflow:release-start\n</code></pre> <p>Acknowledge the proposed version (or change if needed) and then run:</p> <pre><code>./mvnw gitflow:release-finish\n</code></pre>"},{"location":"developer-guide/project-setup.html#milestone-release-management","title":"Milestone / Release Management","text":"<p>After the publication of the new release, it is time to tell the users that you produced a new version. The <code>GitHub release</code> is produced by using the GitHub feature <code>Close Milestone</code>. A special GitHub action is preparing the release notes as a draft. Then click on <code>Publish Release</code> to make it public. This will trigger some GitHub internal notifications and people subscribed to notification about the library will get notified.</p>"},{"location":"developer-guide/project-setup.html#what-modules-get-deployed-to-repository","title":"What modules get deployed to repository","text":"<p>Every Maven module is enabled by default. If you want to change this, please provide the property</p> <pre><code>&lt;maven.deploy.skip&gt;true&lt;/maven.deploy.skip&gt;\n</code></pre> <p>inside the corresponding <code>pom.xml</code>. Currently, all <code>examples</code> are EXCLUDED from publication into Maven Central.</p>"},{"location":"introduction/index.html","title":"Start here","text":"<p>If you are visiting this project for the first time, please check the following sections:</p> <ul> <li>Motivation</li> <li>Features</li> <li>Solution Strategy</li> <li>Further Outlook</li> </ul>"},{"location":"introduction/features.html","title":"Features","text":"<p>Currently, the library supports the following features:</p>"},{"location":"introduction/features.html#general","title":"General","text":"<ul> <li>Ingress Adapters:<ul> <li>Spring Cloud Kafka</li> <li>Axon Framework</li> </ul> </li> <li>MetaData extractors:<ul> <li>Message based (Headers)</li> <li>Channel based (Properties)</li> </ul> </li> <li>Persisting Message Accepting Adapter</li> <li>Message Persistence<ul> <li>In-Memory</li> <li>MyBatis (using the same DB as Camunda Platform 7)</li> </ul> </li> <li>Batch processor running on schedule<ul> <li>Batch modes: all, fail-first</li> </ul> </li> <li>Correlation error strategies: ignore, drop, retry</li> <li>Message Buffering (TTL)</li> <li>Message Expiry</li> <li>Camunda Cockpit Plugin to display the content of the inbox table</li> </ul>"},{"location":"introduction/further-outlook.html","title":"Further Outlook","text":""},{"location":"introduction/further-outlook.html#other-ideas","title":"Other ideas","text":"<ul> <li>More ingress adapters:<ul> <li>Apache Camel</li> <li>JMS</li> <li>REST implementing Camunda API</li> </ul> </li> </ul> <p>Is the library missing a feature important for you?  Please report it.</p>"},{"location":"introduction/motivation.html","title":"Motivation","text":"<p>Correlation is about targeting a running workflow (for example running inside the Camunda Platform 7)  containing the state update by an external system. Inside the Camunda Platform it is important that the  message subscription is present at the time of correlation, otherwise the correlation is mismatched.</p> <p>If you are building a distributed system using the Camunda Platform 7 as a part of it, you should not  make assumptions or assertions regarding the speed of processing of components, message ordering,  message delivery or timings. To make sure that the correlation is not dependent on all those assumptions, the usage of inbox pattern to store the message locally and then deliver it timely on schedule is a good  practise.</p>"},{"location":"introduction/solution.html","title":"Solution","text":"<p>The library provides a core that is responsible for accepting the message, storing it into persistence storage and processing it scheduled. If any errors occur during the correlation, these are handled by one of the pre-configured error strategies, like retry, ignore or drop... </p> <p>In addition, there are a set of several ingress adapters to support different communication technologies.</p>"},{"location":"user-guide/index.html","title":"Overview","text":"<p>The user guide consists of several sections.</p>"},{"location":"user-guide/index.html#concepts-and-components","title":"Concepts and Components","text":"<ul> <li>Architecture</li> <li>Ingress Adapter</li> <li>Message Acceptor</li> <li>Message Persistence</li> <li>Scheduled Processing</li> <li>Metrics</li> </ul>"},{"location":"user-guide/index.html#examples","title":"Examples","text":"<ul> <li>Kafka Example</li> <li>Axon Example</li> </ul>"},{"location":"user-guide/index.html#references","title":"References","text":"<ul> <li>Camunda Community Summit 2022 Talk</li> </ul>"},{"location":"user-guide/examples.html","title":"Examples","text":""},{"location":"user-guide/examples.html#working-example","title":"Working example","text":"<p>We provide examples demonstrating the usage of the library with different messaging technologies. The general example is a \"Travel Reservation\" business process, aiming to book flights and a hotel  in a target city. The reservation process receives the customer name, the source city, the target city  and the dates of the travel. Using this information, it requests the flights by the flight service and  a hotel by the hotel service. The confirmed flight information and hotel information is stored inside  the process payload variables.</p> <p></p> <p>The following sequence diagram illustrates the timing of messages being passed:</p> <p></p> <p>As a result, the <code>HotelReservationConfirmed</code> is received before <code>FlightReservationConfirmed</code> causing a trouble, if message order is strict and must be preserved. In addition, at the time of the receiving of the first message, the process has not completed the long-running task of saving the details.</p> <p>In the following examples we are not focusing on demonstration of features resulting from the  race conditions (resolved faulty), but on purpose created illegal (unwished) message ordering, causing to fail the orchestration by design.</p>"},{"location":"user-guide/examples.html#software-requirements-to-run-examples","title":"Software requirements to run examples","text":"<ul> <li>Docker</li> <li>Docker-Compose</li> <li>KCat (formerly known as KafkaCat)</li> <li>JQ</li> <li>Curl</li> </ul>"},{"location":"user-guide/examples.html#spring-cloud-streams-with-kafka","title":"Spring Cloud Streams with Kafka","text":"<p>The example demonstrates the usage of the library using Kafka for communication. In doing so, we rely on the Spring Cloud Streams binding for Kafka. We constructed an example sending and  receiving data between services using Apache Kafka.</p> <p>To run the examples, you will need to have Docker installed on your machine. Please first run the build of the examples and construct the container images...To do so, please run:</p> <p><code>mvn clean install -Pdocker-assembly -f example</code></p> <p>Then start the provided images using the supplied docker-compose file, by running:</p> <p><code>docker-compose -f example/spring-cloud/docker-compose.yml up -d</code></p> <p>This command will start Apache Kafka, Zookeeper, Flight Service and Hotel Service locally. As a next step, open your IDE and run the  <code>io/holunda/camunda/bpm/example/kafka/TravelAgencyKafkaCorrelationApplication.kt</code> application by  providing the spring profile <code>camunda-correlate</code> (module <code>example/spring-cloud/reservation-kafka</code>).</p> <p>Having it all up-and running, you can send the first message, by using the provided script, which uses  <code>kcat</code>/<code>kafkacat</code> and <code>jq</code>. Please run:</p> <p><code>example/spring-cloud/example.sh reservation</code> </p> <p>to send the message to the reservation topic. As a result, the process should get started, and you should see the messages <code>[SEND BOOK FLIGHT]</code> and <code>[SEND BOOK HOTEL]</code> in your log, indicating that the messages are sent to corresponding topics. The services are executed delayed (2 secs, 5 secs), during the process  is executing the long-running task (saving the reservation details lasts 10 seconds), you will see the response messages coming in. For demonstration purposes, the service delays are configured in a way that the \"expected\" answer by the flight is received after the \"unexpected\" response from the hotel  is received. Therefor, you will see the exception (<code>MismatchedCorrelationException</code>) in the log first.</p> <p>After this, you can inspect the content of the inbox by calling an endpoint http://localhost:8080/admin/list-messages/, as alternative you might open the Camunda Cockpit and check the messages inside the plugin section <code>Correlation</code>.</p>"},{"location":"user-guide/examples.html#axon-events-aka-using-camunda-platform-7-as-microservice-orchestrator","title":"Axon Events aka using Camunda Platform 7 as Microservice Orchestrator","text":"<p>The example demonstrates the usage of the library using Axon Framework / Axon Server based communication. In doing so, We constructed an example sending and receiving data between services using Axon Command and Event Buses.</p> <p>To run the examples, you will need to have Docker installed on your machine. Please first run the build of the examples and construct the container images...To do so, please run:</p> <p><code>mvn clean install -Pdocker-assembly -f example</code></p> <p>Then start the provided images using the supplied docker-compose file, by running:</p> <p><code>docker-compose -f example/axon/docker-compose.yml up -d</code></p> <p>This command will start Axon Server, Flight Service and Hotel Service locally. As a next step, open your IDE and run the <code>io/holunda/camunda/bpm/example/axon/TravelAgencyAxonCorrelationApplication.kt</code> application (module <code>example/axon/reservation-axon</code>).</p> <p>Having it all up-and running, you can send the first message, by using the provided script, which uses <code>curl</code> and <code>jq</code>. Please run:</p> <p><code>example/axon/example.sh reservation</code></p> <p>to send the message to the REST-ful endpoint, which will emit an Axon event. As a result, the process should get started, and you should see the messages <code>[SEND BOOK FLIGHT]</code> and <code>[SEND BOOK HOTEL]</code> in your log, indicating that the commands are sent to. The services are executed delayed (2 secs, 5 secs), during the process is executing the long-running task (saving the reservation details lasts 10 seconds), you will see the response messages coming in. For demonstration purposes, the service delays are configured in a way that the \"expected\" answer by the flight is received after the \"unexpected\" response from the hotel is received. Therefor, you will see the exception (<code>MismatchedCorrelationException</code>) in the log first.</p> <p>After this, you can inspect the content of the inbox by calling an endpoint http://localhost:8080/admin/list-messages/.</p>"},{"location":"user-guide/filtering-messages.html","title":"Message Filter","text":"<p>In messaging scenarios it is not uncommon that the message bus is transporting more types of message than the current system should consume. In this case, it is important to filter out and ignore the irrelevant messages and take only the relevant messages into consideration. In the same time, all messages still needs to be consumed in order not to block the follow-up messages. In order to filter the messages between the Ingress Adapter and the Message Acceptor we supply a special <code>MessageFilter</code> to filter out the supported messaged.</p> <pre><code>/**\n * Message filter to filter messages delivered to the message acceptor.\n */\ninterface MessageFilter {\n/**\n   * Checks if the message should be delivered to the message acceptor.\n   * @param message message instance\n   * @param messageMetaData metadata of the message\n   * @return `true` if message should be accepted.\n   */\nfun &lt;P&gt; accepts(message: AbstractChannelMessage&lt;P&gt;, messageMetaData: MessageMetaData): Boolean\n}\n</code></pre> <p>Feel free to implement your own filters and supply them as a Spring Bean or choose on of the predefined filter and use them in your setup.</p>"},{"location":"user-guide/filtering-messages.html#allmessagefilter","title":"AllMessageFilter","text":"<p>The <code>AllMessageFilter</code> accepts all messages received from the Ingress adapter. </p>"},{"location":"user-guide/filtering-messages.html#andcompositemessagefilter","title":"AndCompositeMessageFilter","text":"<p>The <code>AndCompositeMessageFilter</code> is a composite filter consisting of a list of <code>MessageFilter</code> implementations combined by a logical AND operator.</p>"},{"location":"user-guide/filtering-messages.html#orcompositemessagefilter","title":"OrCompositeMessageFilter","text":"<p>The <code>OrCompositeMessageFilter</code> is a composite filter consisting of a list of <code>MessageFilter</code> implementations combined by a logical OR operator.</p>"},{"location":"user-guide/filtering-messages.html#notmessagefilter","title":"NotMessageFilter","text":"<p>The <code>NotMessageFilter</code> is a filter inverting the application of a supplied <code>MessageFilter</code> implementation.</p>"},{"location":"user-guide/filtering-messages.html#typelistmessagefilter","title":"TypeListMessageFilter","text":"<p>The <code>TypeListMessageFilter</code> is a filter accepting all messages with a payload type being one of the specified types.</p>"},{"location":"user-guide/filtering-messages.html#typeexistsonclasspathmessagefilter","title":"TypeExistsOnClasspathMessageFilter","text":"<p>The <code>TypeExistsOnClasspathMessageFilter</code> is accepting all messages that payload type is available on the class path. </p>"},{"location":"user-guide/ingress-axon.html","title":"Axon Framework Ingress Adapter","text":"<p>The Axon Framework Ingress Adapter is responsible for receiving events on Axon Event bus and sending them for the correlation to the library. </p>"},{"location":"user-guide/ingress-axon.html#message","title":"Message","text":"<p>Axon Event Message is received and deserialized by Axon Framework, using the configured message de-serializer and passed to the ingress adapter. The adapter is reading headers from message <code>MetaData</code> and converts them into message headers. The payload is encoded into serializable payload using  the configured encoder (currently Jackson).</p>"},{"location":"user-guide/ingress-spring-cloud.html","title":"Spring Cloud Ingress Adapter","text":"<p>The Spring Cloud Ingress Adapter is a component responsible for receiving Spring Cloud messages (using configured binding like Kafka, AMQP or others) and  convert them into message format used by the library.</p>"},{"location":"user-guide/ingress-spring-cloud.html#message","title":"Message","text":"<p>Kafka Message is received and the Kafka headers are converted to message headers. </p>"},{"location":"user-guide/ingress.html","title":"Ingress Adapter","text":"<p>The ingress adapter is a component responsible for the adaptation of the communication technology to the uniform message format used in the library. It is responsible for creation the instance of a message including message headers and serialized message payload.</p> <p>The following Ingress Adapter are available out of the box:</p> <ul> <li>Axon Framework</li> <li>Spring Cloud Stream</li> </ul>"},{"location":"user-guide/message-acceptor.html","title":"Message Acceptor","text":"<p>The message acceptor is a component responsible to receive the message from the Ingress adapter and store it into the database. </p>"},{"location":"user-guide/message-acceptor.html#message-metadata","title":"Message Metadata","text":"<p>Along with the payload the message must contain metadata represented by the <code>MessageMetadata</code>. This metadata is extracted from the channel settings,  message payload and other sources. Usually the <code>MessageMetadata</code> instance can't be constructed at one place, so we supply the <code>MessageMetaDataExtractorChain</code> consisting of <code>MessageMetaDataSnippetExtractor</code> instances. By doing so, every aspect of metadata extraction is put in it own class and the result of the  extraction is gathered in one <code>MessageMetadata</code> instance.</p> <p>To simplify the construction of <code>MessageMetaDataExtractorChain</code> we supply several <code>MessageMetaDataSnippetExtractor</code> implementations:</p>"},{"location":"user-guide/message-acceptor.html#channelconfigmessagemetadatasnippetextractor","title":"ChannelConfigMessageMetaDataSnippetExtractor","text":"<p>The <code>ChannelConfigMessageMetaDataSnippetExtractor</code> is responsible for reading metadata from the channel configuration. Usually, channel configuration parameters like message encoding are extracted that way.</p>"},{"location":"user-guide/message-acceptor.html#headermessagemessagemetadatasnippetextractor","title":"HeaderMessageMessageMetaDataSnippetExtractor","text":"<p>The <code>HeaderMessageMessageMetaDataSnippetExtractor</code> is responsible for reading metadata from message headers. Most communication technologies support some concept of message headers and the corresponding Ingress Adapter is mapping those headers to message headers used in the library. By doing so,  you can influence message attributes on a message level. For example, if you are receiving different types of messages by the same ingress adapter, this is  the easiest way to detect the type of the message.</p>"},{"location":"user-guide/message-acceptor.html#message-filtering","title":"Message filtering","text":"<p>Sometimes, more messages are sent through the channel as needed to be received by the Ingress Adapter and needs to be accepted and stored by the library.  For this purpose, a special filtering component <code>MessageFilter</code> is configured in the <code>MessageAcceptor</code>. For more details, how to use the filter please check the Filtering messages section. </p> <p>You can configure what messages it will support and filter the messages that will be delivered to the message acceptor.</p>"},{"location":"user-guide/message-acceptor.html#persisting-channel-message-acceptor","title":"Persisting Channel Message Acceptor","text":"<p>To implement the inbox pattern, the message acceptor stores received messages in a persistent storage. For this purpose, the <code>PersistingChannelMessageAcceptor</code> is implemented. For more details, please check the Message Persistence section.</p>"},{"location":"user-guide/message-persistence.html","title":"Message Persistence","text":"<p>The messages received and accepted by the message acceptor are stored in a relation database. For doing so, we implemented a MyBatis mapper of the underlying entity in order to keep the dependency track as small as possible (MyBatis is a library used and supplied by Camunda Platform 7). For the persistence of the messages the library uses a database table <code>COR_MESSAGE</code> with the following structure:</p> Column Java Datatype JDBC Datatype Description ID String VARCHAR Message id (unique) PAYLOAD_ENCODING String VARCHAR Encoding of the payload PAYLOAD_TYPE_NAMESPACE String VARCHAR Namespace of the payload type, for example package PAYLOAD_TYPE_NAME String VARCHAR Simple type name of the payload type, for example class name PAYLOAD_TYPE_REVISION String VARCHAR Revision of the payload type. PAYLOAD ByteArray BINARY Byte array containing the encoded payload INSERTED Instant TIMESTAMP WITH TIMEZONE Timestamp of message ingestion TTL_DURATION String VARCHAR Time to live of the message as Duration string EXPIRATION Instant TIMESTAMP WITH TIMEZONE Expiration of the message as timestamp RETRIES Integer INTEGER Number of retries of message correlation NEXT_RETRY Instant TIMESTAMP WITH TIMEZONE Timestamp of the next retry ERROR String VARCHAR Last error stacktrace produced during correlation <p>Depending on your database you will need different SQL DDLs to create the underlying DB table. Here are some dialects, we already tried out:</p>"},{"location":"user-guide/message-persistence.html#ms-sql-azure-sql","title":"MS SQL / Azure SQL","text":"<p>We use <code>NVARCHAR</code> as a basic type for strings because of improved index performance for UTF-8 encoded strings.</p> <pre><code>CREATE TABLE COR_MESSAGE (\nID                     NVARCHAR(64) UNIQUE NOT NULL,\nPAYLOAD_ENCODING       NVARCHAR(64)        NOT NULL,\nPAYLOAD_TYPE_NAMESPACE NVARCHAR(128)       NOT NULL,\nPAYLOAD_TYPE_NAME      NVARCHAR(128)       NOT NULL,\nPAYLOAD_TYPE_REVISION  NVARCHAR(64),\nPAYLOAD                BINARY(4096),\nINSERTED               DATETIME2           NOT NULL,\nTTL_DURATION           NVARCHAR(32),\nEXPIRATION             DATETIME2,\nRETRIES                INT                 NOT NULL,\nNEXT_RETRY             DATETIME2,\nERROR                  NVARCHAR(MAX)\n);\n</code></pre>"},{"location":"user-guide/message-persistence.html#h2-hsql","title":"H2 / HSQL","text":"<pre><code>CREATE TABLE COR_MESSAGE (\n    ID                     VARCHAR2(64) UNIQUE NOT NULL,\n    PAYLOAD_ENCODING       VARCHAR2(64)        NOT NULL,\n    PAYLOAD_TYPE_NAMESPACE VARCHAR2(128)       NOT NULL,\n    PAYLOAD_TYPE_NAME      VARCHAR2(128)       NOT NULL,\n    PAYLOAD_TYPE_REVISION  VARCHAR2(64),\n    PAYLOAD                BINARY(4096),\n    INSERTED               TIMESTAMP           NOT NULL,\n    TTL_DURATION           VARCHAR2(32),\n    EXPIRATION             TIMESTAMP,\n    RETRIES                INTEGER             NOT NULL,\n    NEXT_RETRY             TIMESTAMP,\n    ERROR                  CLOB(10000)\n);\n</code></pre>"},{"location":"user-guide/metrics.html","title":"Metrics","text":"<p>In order to be able to monitor the library in operations, we expose different metrics using  standard Spring Boot approach and use Micrometer as the library. By doing so, we provide maximum flexibility for the integration of the metrics in your monitoring system (like Prometheus or others).</p> <p>The following metrics are provided:</p> Component Name Type Tags Description Ingress camunda_bpm_correlate_ingress_received_total counter channel Total number of messages received via given channel. Ingress camunda_bpm_correlate_ingress_accepted_total counter channel Total number of messages accepted via channel. Ingress camunda_bpm_correlate_ingress_ignored_total counter channel Total number of messages received but ignored via channel. Acceptor camunda_bpm_correlate_acceptor_persisted_total counter Total number of messages persisted in the inbox. Acceptor camunda_bpm_correlate_acceptor_dropped_total counter Total number of messages dropped instead of persisting in the inbox. Inbox camunda_bpm_correlate_inbox_message gauge status Number of messages in the inbox by status. The statuses are: total, retrying, in_progress, error, maxRetriesReached and paused. Correlation camunda_bpm_correlate_correlation_success counter Total number of messages successfully correlated. Correlation camunda_bpm_correlate_correlation_error counter Total number of messages correlated with error."},{"location":"user-guide/scheduled-processing.html","title":"Scheduled Processing","text":"<p>After the messages are stored in the database, they get processed triggered by a scheduler. The processing is performed not for every message, but on message batches. A message batch is a collection of messages with the same <code>CorrelationHint</code>. This means that all messages from one batch will be correlated with the same process instance.</p> <p>The configuration of the scheduler is an important setting of the library and influences the time for correlation, error detection and error recovery. The following sections describe the configuration properties controlling the timing of the correlation.</p>"},{"location":"user-guide/scheduled-processing.html#configuration-summary","title":"Configuration summary","text":"<p>Here is a configuration example:</p> <pre><code>correlate:\nbatch:\nmode: all query:    # query scheduler\npollInitialDelay: PT10S\npollInterval: PT6S\ncleanup:  # cleanup of expired messages\npollInitialDelay: PT1M\npollInterval: PT1M\npersistence: # persistence setting\nmessageMaxRetries: 100 messageFetchPageSize: 100\nmessageBatchSize: 1\nretry:\nretryMaxBackoffMinutes: 5 retryBackoffBase: 2.0 </code></pre> Property Values Meaning Default batch.mode <code>all</code>, <code>fail_first</code> Batch processing mode all batch.query.pollInitialDelay Duration in ISO8601 Start delay before correlation scheduler starts PT10S batch.query.pollInterval Duration in ISO8601 Delay between correlation attempts PT6S batch.cleanup.pollInitialDelay Duration in ISO8601 Start delay before clean-up scheduler starts batch.cleanup.pollInterval Duration in ISO8601 Delay between clean-ups persistence.messageMaxRetries Integer Maximum retries before giving up correlation 100 persistence.messageFetchPageSize Integer Paging size by message fetch 100 persistence.messageBatchSize Integer Limit the number of messages processed from a batch -1 retry.retryMaxBackoffMinutes Integer Maximum backoff-time  in minutes 180 retry.retryBackoffBase Float Base for exponential backoff-time 180"},{"location":"user-guide/scheduled-processing.html#reading-message","title":"Reading message","text":"<p>Messages are read in batches which are paged. You can set-up the page size, the interval between reads and the initial delay from the application start.</p>"},{"location":"user-guide/scheduled-processing.html#batch-correlation","title":"Batch correlation","text":"<p>Batches of messages are checked to fulfill the following criteria:</p> <ul> <li>Batch contains no messages with errors</li> <li>Batch contains messages with errors and all those are due to retry (now &lt; due, retry &lt; max-retries)</li> </ul> <p>Messages of one batch are correlated in order of their sorting. If a correlation error occurs, the batch correlation is either interrupted (<code>fail_first</code> mode) or the batch is correlated to the end (<code>all</code> mode).</p> <p>An important parameter for batch processing is the <code>message-batch-size</code>. This parameter specifies the number of messages taken from a batch for synchronous correlation. Effectively, this parameter has two interesting values. Set this parameter to <code>-1</code> (default) and all messages from one batch will be correlated directly one after another. Set this parameter to <code>1</code> and the batch will be constructed, but only the first message will be correlated in current run. If successful, the  next message will be fetched during the next message query (after the <code>batch.query.pollInterval</code>, which should be a small interval). By doing so, you can deal with asynchronous continuations in your process.</p>"},{"location":"user-guide/scheduled-processing.html#error-detection","title":"Error detection","text":"<p>If the error is detected during the correlation, it is handled by the library. If the message time-to-live is set and the error happens during the TTL (the message is alive), the error is not noted (and not stored), but the message will be skipped and picked up by the next batch correlation. If the error happens after the message TTL or TTL is not set, the error is noted causing the following information to be stored along the message in the database:</p> <ul> <li>head of the exception stack trace occurred during the correlation</li> <li>value incremented by 1 in <code>attempt</code></li> <li>new due date for retry (now plus value in minutes of <code>retryBackoffBase</code> at the power of <code>attempt</code> but at most the <code>retryMaxBackOffMinutes</code>)</li> </ul>"},{"location":"user-guide/scheduled-processing.html#message-processing-example","title":"Message processing example","text":"<p>Imagine the message inserted at a point in time with TTL of 10 seconds producing a correlation error which can't be resolved by retries. Imagine that the value of <code>retryMaxBackOffMinutes</code> is set to <code>10</code> and the <code>messageMaxRetries</code> is <code>5</code>.</p> Offset from ingested (sec) Why Attempt Next Retry from ingested (secs) 6 Picked up by batch correlation scheduler, error, no error recording because of TTL 0 null 12 Picked up by batch correlation scheduler, error, error noted 1 12sec offset + 2^0M = 12 + 60 = 72 18 Not picked up, because of error and next retry not due 1 72 72 Picked up by batch correlation scheduler, error, error noted 2 72sec offset + 2^1M = 72 + 120 = 192 192 Picked up by batch correlation scheduler, error, error noted 3 192sec offset + 2^2M = 192 + 240 = 432 432 Picked up by batch correlation scheduler, error, error noted 4 432sec offset + 2^3M = 432 + 480 = 922, but 600 sec is max = 600 600 Picked up by batch correlation scheduler, error, error noted 5 600 606 Not picked up, because of error and max retries are reached 5 600"},{"location":"user-guide/scheduled-processing.html#running-in-a-cluster","title":"Running in a cluster","text":"<p>For activation of the cluster support, please add the following configuration snippet to your <code>application.yml</code>:</p> <pre><code>correlate:\nbatch:\ncluster:\nenabled: true\nqueuePollLockMostInterval: PT5M\n</code></pre> <p>For a cluster operations it is important to synchronize the batch schedulers between the cluster nodes. For this purpose, the library Shedlock is used. Shedlock synchronizes the scheduled tasks using a RDBMS table (we are using a JDBC Lock Provider). Here are the required DDL snippets for some common databases, please see shedlock documentation for more information.</p> <pre><code>CREATE TABLE shedlock\n(\nname       NVARCHAR(64)  NOT NULL,\nlock_until DATETIME2     NOT NULL,\nlocked_at  DATETIME2     NOT NULL,\nlocked_by  NVARCHAR(255) NOT NULL,\nPRIMARY KEY (name)\n);\n</code></pre> <pre><code>CREATE TABLE shedlock\n(\n    name       VARCHAR(64)  NOT NULL,\n    lock_until DATETIME2     NOT NULL,\n    locked_at  DATETIME2     NOT NULL,\n    locked_by  VARCHAR(255) NOT NULL,\n    PRIMARY KEY (name)\n);\n</code></pre>"}]}