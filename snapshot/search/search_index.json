{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Why should I use this? # Imagine you integrate your Camunda Engine into a larger application landscape. In doing so the inter-system communication becomes important and questions on communication styles and patterns arise. In the world of self-contained systems, the asynchronous communication with messages is wide adopted. This library helps you to solve integration problems around correlation of messages with processes. How to start? # A good starting point is the Getting Started section, but then go on through the and have a look Introduction and check out our Working Examples . If you need more details on usage and configuration, check the User Guide . Get in touch # If you are missing a feature, have a question regarding usage or deployment, you should definitely get in touch with us. There are various ways to do so:","title":"Home"},{"location":"index.html#why-should-i-use-this","text":"Imagine you integrate your Camunda Engine into a larger application landscape. In doing so the inter-system communication becomes important and questions on communication styles and patterns arise. In the world of self-contained systems, the asynchronous communication with messages is wide adopted. This library helps you to solve integration problems around correlation of messages with processes.","title":"Why should I use this?"},{"location":"index.html#how-to-start","text":"A good starting point is the Getting Started section, but then go on through the and have a look Introduction and check out our Working Examples . If you need more details on usage and configuration, check the User Guide .","title":"How to start?"},{"location":"index.html#get-in-touch","text":"If you are missing a feature, have a question regarding usage or deployment, you should definitely get in touch with us. There are various ways to do so:","title":"Get in touch"},{"location":"getting-started.html","text":"Install Dependency # First install the extension using the corresponding ingres adapter (in this example we use Kafka): <properties> <camunda-bpm-correlate.version> 0.0.1 </camunda-bpm-correlate.version> </properties> <dependencies> <dependency> <groupId> io.holunda </groupId> <artifactId> camunda-bpm-correlate-spring-cloud-stream </artifactId> <version> ${camunda-bpm-correlate.version} </version> </dependency> <dependency> <groupId> org.springframework.cloud </groupId> <artifactId> spring-cloud-stream-binder-kafka </artifactId> </dependency> </dependencies> Configuration # Configure your basic Spring Cloud Streams Kafka configuration to looks like this (or similar). Important is the name of the function definition. spring : cloud : stream : function : definition : streamByteMessageConsumer bindings : streamByteMessageConsumer-in-0 : correlate-ingres-binding bindings : correlate-ingres-binding : content-type : application/json destination : ${KAFKA_TOPIC_CORRELATE_INGRES:correlate-ingres} binder : correlate-ingres-binder group : ${KAFKA_GROUP_ID} binders : correlate-ingres-binder : type : kafka defaultCandidate : false inheritEnvironment : false environment : spring : kafka : consumer : key-deserializer : org.apache.kafka.common.serialization.ByteArrayDeserializer value-deserializer : org.apache.kafka.common.serialization.ByteArrayDeserializer cloud : stream : kafka : binder : autoCreateTopics : false autoAddPartitions : false brokers : ${KAFKA_BOOTSTRAP_SERVER_HOST:localhost}:${KAFKA_BOOTSTRAP_SERVER_PORT:9092} configuration : security.protocol : ${KAFKA_SECURITY_PROTOCOL_OVERRIDE:PLAINTEXT} In addition, add the configuration of the extension: correlate : enabled : true channels : stream : channelEnabled : true message : timeToLiveAsString : PT10S # errors during TTL seconds after receiving are ignored payloadEncoding : jackson # our bytes are actually JSON written by Jackson. batch : mode : all # default fail_first -> 'all' will correlate one message after another, resulting in ignoring the order of receiving query : # query scheduler pollInitialDelay : PT10S pollInterval : PT6S cleanup : # cleanup of expired messages pollInitialDelay : PT1M pollInterval : PT1M persistence : messageMaxRetries : 5 # default 100 -> will try to deliver 5 times at most messageFetchPageSize : 100 # default 100 retry : retryMaxBackoffMinutes : 5 # default 180 -> maximum 5 minutes between retries retryBackoffBase : 2.0 # value in minutes default 2.0 -> base in the power of retry to calculate the next retry","title":"Getting started"},{"location":"getting-started.html#install-dependency","text":"First install the extension using the corresponding ingres adapter (in this example we use Kafka): <properties> <camunda-bpm-correlate.version> 0.0.1 </camunda-bpm-correlate.version> </properties> <dependencies> <dependency> <groupId> io.holunda </groupId> <artifactId> camunda-bpm-correlate-spring-cloud-stream </artifactId> <version> ${camunda-bpm-correlate.version} </version> </dependency> <dependency> <groupId> org.springframework.cloud </groupId> <artifactId> spring-cloud-stream-binder-kafka </artifactId> </dependency> </dependencies>","title":"Install Dependency"},{"location":"getting-started.html#configuration","text":"Configure your basic Spring Cloud Streams Kafka configuration to looks like this (or similar). Important is the name of the function definition. spring : cloud : stream : function : definition : streamByteMessageConsumer bindings : streamByteMessageConsumer-in-0 : correlate-ingres-binding bindings : correlate-ingres-binding : content-type : application/json destination : ${KAFKA_TOPIC_CORRELATE_INGRES:correlate-ingres} binder : correlate-ingres-binder group : ${KAFKA_GROUP_ID} binders : correlate-ingres-binder : type : kafka defaultCandidate : false inheritEnvironment : false environment : spring : kafka : consumer : key-deserializer : org.apache.kafka.common.serialization.ByteArrayDeserializer value-deserializer : org.apache.kafka.common.serialization.ByteArrayDeserializer cloud : stream : kafka : binder : autoCreateTopics : false autoAddPartitions : false brokers : ${KAFKA_BOOTSTRAP_SERVER_HOST:localhost}:${KAFKA_BOOTSTRAP_SERVER_PORT:9092} configuration : security.protocol : ${KAFKA_SECURITY_PROTOCOL_OVERRIDE:PLAINTEXT} In addition, add the configuration of the extension: correlate : enabled : true channels : stream : channelEnabled : true message : timeToLiveAsString : PT10S # errors during TTL seconds after receiving are ignored payloadEncoding : jackson # our bytes are actually JSON written by Jackson. batch : mode : all # default fail_first -> 'all' will correlate one message after another, resulting in ignoring the order of receiving query : # query scheduler pollInitialDelay : PT10S pollInterval : PT6S cleanup : # cleanup of expired messages pollInitialDelay : PT1M pollInterval : PT1M persistence : messageMaxRetries : 5 # default 100 -> will try to deliver 5 times at most messageFetchPageSize : 100 # default 100 retry : retryMaxBackoffMinutes : 5 # default 180 -> maximum 5 minutes between retries retryBackoffBase : 2.0 # value in minutes default 2.0 -> base in the power of retry to calculate the next retry","title":"Configuration"},{"location":"developer-guide/contribution.html","text":"There are several ways in which you may contribute to this project. File issues Submit a pull requests Found a bug or missing feature? # Please file an issue in our issue tracking system. Submit a Pull Request # If you found a solution to an open issue and implemented it, we would be happy to add your contribution in the code base. For doing so, please create a pull request. Prior to that, please make sure you rebased against the develop branch stick to project coding conventions added test cases for the problem you are solving added docs, describing the change generally comply with codeacy report","title":"Contributing"},{"location":"developer-guide/contribution.html#found-a-bug-or-missing-feature","text":"Please file an issue in our issue tracking system.","title":"Found a bug or missing feature?"},{"location":"developer-guide/contribution.html#submit-a-pull-request","text":"If you found a solution to an open issue and implemented it, we would be happy to add your contribution in the code base. For doing so, please create a pull request. Prior to that, please make sure you rebased against the develop branch stick to project coding conventions added test cases for the problem you are solving added docs, describing the change generally comply with codeacy report","title":"Submit a Pull Request"},{"location":"developer-guide/project-setup.html","text":"If you are interested in developing and building the project please read the following the instructions carefully. Version control # To get sources of the project, please execute: git clone https://github.com/holunda-io/camunda-bpm-correlate.git cd camunda-rest-client-spring-boot We are using gitflow in our git SCM for naming branches. That means that you should start from develop branch, create a feature/<name> out of it and once it is completed create a pull request containing it. Please squash your commits before submitting and use semantic commit messages, if possible. Project Build # Perform the following steps to get a development setup up and running. ./mvnw clean install Integration Tests # By default, the build command will ignore the run of failsafe Maven plugin executing the integration tests (usual JUnit tests with class names ending with ITest). In order to run integration tests, please call from your command line: ./mvnw -Pitest Project build modes and profiles # Documentation # We are using MkDocs for generation of a static site documentation and rely on markdown as much as possible. Note If you want to develop your docs in 'live' mode, run mkdocs serve and access the http://localhost:8000/ from your browser. For creation of documentation, please run: Generation of JavaDoc and Sources # By default, the sources and javadoc API documentation are not generated from the source code. To enable this: ./mvnw clean install -Prelease -Dgpg.skip = true Continuous Integration # GitHub Actions are building all branches on commit hook (for codecov). In addition, a GitHub Actions are used to build PRs and all branches. Release Management # The release is produced by using the GitHub feature \"Close Milestone\". A special GitHub action is preparing the release notes as a draft. Then click on \"Publish Release\" to make it public. What modules get deployed to repository # Every Maven module is enabled by default. If you want to change this, please provide the property <maven.deploy.skip> true </maven.deploy.skip> inside the corresponding pom.xml . Currently, all examples are EXCLUDED from publication into Maven Central.","title":"Project Setup"},{"location":"developer-guide/project-setup.html#version-control","text":"To get sources of the project, please execute: git clone https://github.com/holunda-io/camunda-bpm-correlate.git cd camunda-rest-client-spring-boot We are using gitflow in our git SCM for naming branches. That means that you should start from develop branch, create a feature/<name> out of it and once it is completed create a pull request containing it. Please squash your commits before submitting and use semantic commit messages, if possible.","title":"Version control"},{"location":"developer-guide/project-setup.html#project-build","text":"Perform the following steps to get a development setup up and running. ./mvnw clean install","title":"Project Build"},{"location":"developer-guide/project-setup.html#integration-tests","text":"By default, the build command will ignore the run of failsafe Maven plugin executing the integration tests (usual JUnit tests with class names ending with ITest). In order to run integration tests, please call from your command line: ./mvnw -Pitest","title":"Integration Tests"},{"location":"developer-guide/project-setup.html#project-build-modes-and-profiles","text":"","title":"Project build modes and profiles"},{"location":"developer-guide/project-setup.html#documentation","text":"We are using MkDocs for generation of a static site documentation and rely on markdown as much as possible. Note If you want to develop your docs in 'live' mode, run mkdocs serve and access the http://localhost:8000/ from your browser. For creation of documentation, please run:","title":"Documentation"},{"location":"developer-guide/project-setup.html#generation-of-javadoc-and-sources","text":"By default, the sources and javadoc API documentation are not generated from the source code. To enable this: ./mvnw clean install -Prelease -Dgpg.skip = true","title":"Generation of JavaDoc and Sources"},{"location":"developer-guide/project-setup.html#continuous-integration","text":"GitHub Actions are building all branches on commit hook (for codecov). In addition, a GitHub Actions are used to build PRs and all branches.","title":"Continuous Integration"},{"location":"developer-guide/project-setup.html#release-management","text":"The release is produced by using the GitHub feature \"Close Milestone\". A special GitHub action is preparing the release notes as a draft. Then click on \"Publish Release\" to make it public.","title":"Release Management"},{"location":"developer-guide/project-setup.html#what-modules-get-deployed-to-repository","text":"Every Maven module is enabled by default. If you want to change this, please provide the property <maven.deploy.skip> true </maven.deploy.skip> inside the corresponding pom.xml . Currently, all examples are EXCLUDED from publication into Maven Central.","title":"What modules get deployed to repository"},{"location":"introduction/index.html","text":"If you are visiting this project for the first time, please check the following sections: Motivation Features Solution Strategy Further Outlook","title":"Start here"},{"location":"introduction/features.html","text":"The library supports the following features: General # Ingres Adapters: Spring Cloud Kafka MetaData extractors: Channel based (Headers) Properties Persisting Message Accepting Adapter Message Persistence In-Memory MyBatis (using the same DB as Camunda Platform 7) Batch processor running on schedule Batch modes: all, fail-first Correlation error strategies: ignore, drop, retry Message Buffering (TTL) Message Expiry","title":"Features"},{"location":"introduction/features.html#general","text":"Ingres Adapters: Spring Cloud Kafka MetaData extractors: Channel based (Headers) Properties Persisting Message Accepting Adapter Message Persistence In-Memory MyBatis (using the same DB as Camunda Platform 7) Batch processor running on schedule Batch modes: all, fail-first Correlation error strategies: ignore, drop, retry Message Buffering (TTL) Message Expiry","title":"General"},{"location":"introduction/further-outlook.html","text":"Other ideas # Plugin for Camunda Cockpit More metrics for Prometheus More ingres adapters: Apache Camel REST implementing Camunda API Is the library missing a feature important for you? Please report it .","title":"Further Outlook"},{"location":"introduction/further-outlook.html#other-ideas","text":"Plugin for Camunda Cockpit More metrics for Prometheus More ingres adapters: Apache Camel REST implementing Camunda API Is the library missing a feature important for you? Please report it .","title":"Other ideas"},{"location":"introduction/motivation.html","text":"Correlation is about targeting a running workflow (for example running inside the Camunda Platform 7) containing the state update by an external system. Inside the Camunda Platform it is important that the message subscription is present at the time of correlation, otherwise the correlation is mismatched. If you are building a distributed system using the Camunda Platform 7 as a part of it, you should not make assumptions or assertions regarding the speed of processing of components, message ordering, message delivery or timings. To make sure that the correlation is not dependent on all those assumptions, the usage of inbox pattern to store the message locally and then deliver it timely on schedule is a good practise.","title":"Motivation"},{"location":"introduction/solution.html","text":"The library provides a core that is responsible for accepting the message, storing it into persistence storage and processing it scheduled. If any errors occur during the correlation, these are handled by one of the pre-configured error strategies, like retry, ignore or drop... In addition, there are a set of several ingres adapters to support different communication technologies.","title":"Solution"},{"location":"user-guide/index.html","text":"The user guide consists of several sections. Configuration # Examples # Kafka Example","title":"Overview"},{"location":"user-guide/index.html#configuration","text":"","title":"Configuration"},{"location":"user-guide/index.html#examples","text":"Kafka Example","title":"Examples"},{"location":"user-guide/examples.html","text":"Working example # We provide examples demonstrating the usage of the library with different messaging technologies. The general example is a \"Travel Reservation\" business process, aiming to book flights and a hotel in a target city. The reservation process receives the customer name, the source city, the target city and the dates of the travel. Using this information, it requests the flights by the flight service and a hotel by the hotel service. The confirmed flight information and hotel information is stored inside the process payload variables. The following sequence diagram illustrates the timing of messages being passed: As a result, the HotelReservationConfirmed is received before FlightReservationConfirmed causing a trouble, if message order is strict and must be preserved. In addition, at the time of the receiving of the first message, the process has not completed the long-running task of saving the details. In the following examples we are not focusing on demonstration of features resulting from the race conditions (resolved faulty), but on purpose created illegal (unwished) message ordering, causing to fail the orchestration by design. Software requirements to run examples # Docker Docker-Compose KCat (formerly known as KafkaCat) JQ Spring Cloud Streams with Kafka # The example demonstrates the usage of the library using Kafka for communication. In doing so, we rely on the Spring Cloud Streams binding for Kafka. We constructed an example sending and receiving data between services using Apache Kafka. To run the examples, you will need to have Docker installed on your machine. Please first run the build of the examples and construct the container images...To do so, please run: mvn clean install -Pdocker-assembly -f examples Then start the provided images using the supplied docker-compose file, by running: docker-compose -f examples/spring-cloud-stream-kafka/docker-compose.yml up -d This command will start Apache Kafka, Zookeeper, Flight Service and Hotel Service locally. As a next step, open your IDE and run the io/holunda/camunda/bpm/example/kafka/TravelAgencyKafkaCorrelationApplication.kt application by providing the spring profile camunda-correlate . Having it all up-and running, you can send the first message, by using the provided script, which uses kcat / kafkacat and jq libraries. Please run: examples/spring-cloud-stream-kafka/example.sh reservation to send the message to the reservation topic. As a result, the process should get started, and you should see the messages [SEND BOOK FLIGHT] and [SEND BOOK HOTEL] in your log, indicating that the messages are sent to corresponding topics. The services are executed delayed (2 secs, 5 secs), during the process is executing the long-running task (saving the reservation details lasts 10 seconds), you will see the response messages coming in. For demonstration purposes, the service delays are configured in a way that the \"expected\" answer by the flight is received after the \"unexpected\" response from the hotel is received. Therefor, you will see the exception (MismatchedCorrelationException) in the log first. After this, you can inspect the content of the inbox by calling an endpoint http://localhost:8080/admin/list-messages/ .","title":"Examples"},{"location":"user-guide/examples.html#working-example","text":"We provide examples demonstrating the usage of the library with different messaging technologies. The general example is a \"Travel Reservation\" business process, aiming to book flights and a hotel in a target city. The reservation process receives the customer name, the source city, the target city and the dates of the travel. Using this information, it requests the flights by the flight service and a hotel by the hotel service. The confirmed flight information and hotel information is stored inside the process payload variables. The following sequence diagram illustrates the timing of messages being passed: As a result, the HotelReservationConfirmed is received before FlightReservationConfirmed causing a trouble, if message order is strict and must be preserved. In addition, at the time of the receiving of the first message, the process has not completed the long-running task of saving the details. In the following examples we are not focusing on demonstration of features resulting from the race conditions (resolved faulty), but on purpose created illegal (unwished) message ordering, causing to fail the orchestration by design.","title":"Working example"},{"location":"user-guide/examples.html#software-requirements-to-run-examples","text":"Docker Docker-Compose KCat (formerly known as KafkaCat) JQ","title":"Software requirements to run examples"},{"location":"user-guide/examples.html#spring-cloud-streams-with-kafka","text":"The example demonstrates the usage of the library using Kafka for communication. In doing so, we rely on the Spring Cloud Streams binding for Kafka. We constructed an example sending and receiving data between services using Apache Kafka. To run the examples, you will need to have Docker installed on your machine. Please first run the build of the examples and construct the container images...To do so, please run: mvn clean install -Pdocker-assembly -f examples Then start the provided images using the supplied docker-compose file, by running: docker-compose -f examples/spring-cloud-stream-kafka/docker-compose.yml up -d This command will start Apache Kafka, Zookeeper, Flight Service and Hotel Service locally. As a next step, open your IDE and run the io/holunda/camunda/bpm/example/kafka/TravelAgencyKafkaCorrelationApplication.kt application by providing the spring profile camunda-correlate . Having it all up-and running, you can send the first message, by using the provided script, which uses kcat / kafkacat and jq libraries. Please run: examples/spring-cloud-stream-kafka/example.sh reservation to send the message to the reservation topic. As a result, the process should get started, and you should see the messages [SEND BOOK FLIGHT] and [SEND BOOK HOTEL] in your log, indicating that the messages are sent to corresponding topics. The services are executed delayed (2 secs, 5 secs), during the process is executing the long-running task (saving the reservation details lasts 10 seconds), you will see the response messages coming in. For demonstration purposes, the service delays are configured in a way that the \"expected\" answer by the flight is received after the \"unexpected\" response from the hotel is received. Therefor, you will see the exception (MismatchedCorrelationException) in the log first. After this, you can inspect the content of the inbox by calling an endpoint http://localhost:8080/admin/list-messages/ .","title":"Spring Cloud Streams with Kafka"}]}